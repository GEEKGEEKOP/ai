{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GEEKGEEKOP/ai/blob/main/Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python file.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDNpIYdQ-DY7",
        "outputId": "1beb47b8-0242-4757-d55f-12e20004431e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-25 18:09:26.096927: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-02-25 18:09:26.117040: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-02-25 18:09:26.123086: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-25 18:09:26.137317: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-02-25 18:09:27.436568: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "{'train_runtime': 43.1343, 'train_samples_per_second': 4.034, 'train_steps_per_second': 2.017, 'train_loss': 1.5885062382139008, 'epoch': 3.0}\n",
            "100% 87/87 [00:37<00:00,  2.29it/s]\n",
            "سلام\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "سلام بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد\n",
            "------------------------------------\n",
            "خوبی؟\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "خوبی؟\n",
            "𝓛𝓞𝓝𝓔𝓛𝓨 𝓑𝓞𝓨 هفژژلژ: با با با با با با با با با با با با با با با با با\n",
            "------------------------------------\n",
            "پایتون\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "پایتون بشرش بشرش بشرش بشرش بشرش بشرش بشرش بشرش بشرش بشرش بشرش بشرش بشرش �\n",
            "------------------------------------\n",
            "پارسا\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "پارسان بشرزین بشرزین بشرزین بشرزین بشرزین بشرزین بشرزین بشرزین بشرزین ب�\n",
            "------------------------------------\n",
            "جنده\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "جنده بخود بخود بخود بخود بخود بخود بخود بخود بخود بخود بخود بخود بخود بخود بخود بخو\n",
            "------------------------------------\n",
            "پگاه\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "پگاه بشید بشید بشید بشید بشید بشید بشید بشید بشید بشید بشید بشید بشید ب�\n",
            "------------------------------------\n",
            "چرا تکرار میکنی \n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "چرا تکرار میکنی کرار بشران بشران بشران بشران بشران بشران بشران بشران بشران بشران بشر\n",
            "------------------------------------\n",
            "بخور\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "بخور بخور بخور بخور بخور بخور بخور بخور بخور بخور بخور بخور بخور بخور بخور بخور بخو\n",
            "------------------------------------\n",
            "بگو\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "بگون بشریز\n",
            "𝓛𝓞𝓝𝓔𝓛𝓨 𝓑𝓞𝓨 هفژژلژ: بشریز\n",
            "𝓛𝓞𝓝𝓔𝓛𝓨 𝓑𝓞𝓨 هفژ�\n",
            "------------------------------------\n",
            "parsa\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "parsa: 𝕽𝖆𝖍𝖎𝖒: 𝕽𝖆𝖍𝖎𝖒: 𝕽𝖆𝖍𝖎𝖒: 𝕽𝖆𝖍𝖎𝖒: 𝕽𝖆𝖍𝖎𝖒: 𝕽𝖆𝖍�\n",
            "------------------------------------\n",
            "درود\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "درود بشرود بشرود بشرود بشرود بشرود بشرود بشرود بشرود بشرود بشرود بشرود بشرود بشرود بشر\n",
            "------------------------------------\n",
            "بشر\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "بشر بشد بشد بشد بشد بشد بشد بشد بشد بشد بشد بشد بشد بشد بشد بشد بشد بشد بشد بشد �\n",
            "------------------------------------\n",
            "شششش\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "شششششششششششششششششششششششششششششششششششششششششششششششششش\n",
            "------------------------------------\n",
            "عربی\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "عربی بشری بشری بشری بشری بشری بشری بشری بشری بشری بشری بشری بشری بشری بش\n",
            "------------------------------------\n",
            "66\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "66.\n",
            "\n",
            "The following are some of the most popular and popular posts on the site:\n",
            "\n",
            "1.\n",
            "\n",
            "2.\n",
            "\n",
            "3.\n",
            "\n",
            "4.\n",
            "\n",
            "5.\n",
            "\n",
            "6.\n",
            "\n",
            "7.\n",
            "\n",
            "8.\n",
            "\n",
            "9.\n",
            "\n",
            "10.\n",
            "\n",
            "11.\n",
            "\n",
            "12.\n",
            "\n",
            "13.\n",
            "\n",
            "14.\n",
            "\n",
            "15.\n",
            "\n",
            "16.\n",
            "\n",
            "17.\n",
            "\n",
            "18.\n",
            "\n",
            "19.\n",
            "\n",
            "20.\n",
            "\n",
            "------------------------------------\n",
            "ابو طالبی\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "ابو طالبیر بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد بشرد\n",
            "------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}